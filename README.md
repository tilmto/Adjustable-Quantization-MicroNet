# Adjustable-Quantization-MicroNet
#### This is the submission for MicroNet Challenge hosted at NIPS 2019.

### Team
Yonggan Fu, Ruiyang Zhao, Yue Wang, Zhangyang Wang, Yingyan Lin

### Solution
Our method is called **Adjustable Quantization**, a finegrained mix-precision quantization scheme which is extremely fast to reach convergence started from a pretrained float32 model.

Metric: 0.187445 @ 75.064% Top 1 accuracy on ImageNet

### Main Idea: Adjustable Quantization Range + Adjustable Precision
To enable a finegrained mix-precision quantization on a light-weight model, we aim at refining the traditional quantization to be more adjustable according to the efficiency limitation. Based on the previous [Quantization Aware Training](https://arxiv.org/abs/1712.05877), we introduce channel-wise trainable scale factors for both quantization range and precision. To eliminate the effect of the [outliners](https://arxiv.org/abs/1803.08607), we introduce a trainable scale factor ***alpha*** for each channel to make the quantization range tp adjust itself during the quantization process. In addition, to further explore the compression potential, we give a channel-wise scale factor ***beta*** to the quantization precision so that each channel in different layers can learn to adjust its precision. Since number of parameters and FLOPS are all related with the channel precision settings, we explicitly introduce the #Params and #Flops into the loss function, controlling the trade-off between accuracy and efficiency. The quantization parameter in the [QAT paper](https://arxiv.org/abs/1712.05877) can be rewritten as ([ ] means rounding operation):  

<div align="center">
<img src="https://latex.codecogs.com/svg.latex?\Large&space;S=\frac{\alpha*T_{range}}{2^{[\beta*n]}-1}" title="Method" width=150 />
</div>

We choose [EfficientNet-B0](https://arxiv.org/abs/1905.11946), a light-weight neural network generated by Neural Network Search. We apply our Adjustable Quantization (both adjustable quantization range and adjustable precision) on all the weights in the network, and the input activations (feature maps) of all the convolutional layers (including regular convolution and depthwise convolution). And for the input activations of all the swish operation, i.e. *x\*sigmoid(x)* which is the activation function in EfficientNet-B0, we use fixed precision (8-bit) and adjustable quantization range. Therefore, all the weights and activations in EfficientNet-B0 are quantized to an integer while maintaining the potential of high accuracy. 

We introduce [Knowledge Distillation](https://arxiv.org/abs/1503.02531) and SWA ([Stochastic Weight Average](https://arxiv.org/abs/1904.11943)) in the training process for fast convergence. Our loss function contains 3 parts: classification loss, knowledge distillation loss which is the MSE between the output of the original float32 EfficientNet-B0 and the output of quantized EfficientNet-B0, the metric loss for controlling the trade-off between accuracy and model size(#Params and #Flops). To achieve a better convergence, we also apply an iterative training strategy, i.e. dividing every 1/10 epoch into a mini-epoch and only training with Adjustable Precision in 1 mini-epoch every 2 mini-epochs (fixed the precision in the other mini-epoch). This training scheme slows down the descent speed of the metric and helps maintain the accuracy during training process.

To enable a better initialization of EfficientNet-B0 before applying Adjustable Quantization, we first conduct tensorflow official Quantization Aware Training (QAT) to get the initial weights and initial quantization range. To be more specific, we implement EfficientNet-B0 structure in tensorflow [Slim](https://github.com/tensorflow/models/tree/master/research/slim) training package which has good support for QAT. Since the [official pretrained checkpoint](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet) of EfficientNet-B0 provided by the author uses TPU API, we write a script to convert the official pretrained checkpoint into our Slim-based model's checkpoint so that we can enable QAT directly within the Slim framework. After several epochs QAT, we can use the weight and quantization range of the final model as the initialization for further Adjustable Quantization. 


### File Utilization
**adj_quant** : root directory containing all the source codes of Adjustable Quantization
**adj_quant/pretrained** : pretrained float32 Slim-based EfficientNet-B0, and EfficientNet-B0 after Quantization Aware Training
**adj_quant/logs** : tensorboard information collected during training
**adj_quant/weights** : checkpoints saved during training
**adj_quant/best_ckpt** : current best checkpoint with challenge metric 0.187445 @ 75.064% Top 1 accuracy on ImageNet 
**adj_quant/initial_thresholds.json** : the intial min/max quantization range of the each channel in weights and activations 
**adj_quant/model_info.json** : the Params/Flops of each layer in EfficientNet-B0
**adj_quant/data.py** : read training/validation images in tfrecord format
**adj_quant/imagenet.py** : necessary functions for reading ImageNet
**adj_quant/dataset_utils.py** : necessary function for reading ImageNet
**adj_quant/prepare_weights.py** : extract the weights from a pretrained Slim-based model to a pickle file as the initialization of Adjustable Quantization
**adj_quant/train.py** : train EfficientNet-B0 with Adjustable Quantization
**adj_quant/eval.py** : evaluate the accuracy and metric a saved checkpoint file
**adj_quant/scripts/effnetb0** : build model with Adjustable Quantization Nodes
**adj_quant/scripts/trainer** : training strategy and evaluation strategy
**adj_quant/scripts/thresholds.py** : fetch initial quantization range

### How to evaluate the accuracy and metric
To know the usage of command line parameters, please use:
```
python train.py --help
```
or
```
python eval.py --help
```

#### Environment
Tensorflow-gpu 1.13.1, CUDA 10.0, python 3.6.0

#### Evaluation
To check the accuracy and metric of the provided checkpoint, use the following commands:
```
python eval.py --weight_bits 8 \
               --act_bits 8 \
               --swish_bits 8 \
               --batch_size 50 \
               --bits_trainable \
               --ckpt_path best_ckpt/ckpt_metric_0.187445.ckpt \
               --dataset path_to_imagenet_tfrecord
```
***weight_bits*** and ***act_bits*** are the initial precision of the channels in each layer, and it will be multiplied with a channel-wise trainable scale factor saved in the checkpoint file and rounded to a integer to be the final precision of each channel in each layer. ***swish_bits*** is the precision for the input activation of swish operation. ***bits_trainable*** is used to enable Adjustable Precision. ***ckpt_path*** specifies the checkpoint file to be evaluated and you also need to specify the path to ImageNet dataset in tfrecord format. 

After correctly executing **eval.py**, you will get the accuracy, metric, #Params and #Flops shown in your command line. In addition, it will generate a new file named **quant_info.json**, containing the precision of each channel in all the weights and activations. You can recompute the metric according to this file.

#### Training
We provided the pretrained float32 EfficientNet-B0 checkpoint and the quantized EfficientNet-B0 checkpoint after QAT in *.pickle* format in the **pretrained** directory and the initial quantization range in **initial_thresholds.json**. You can directly utilize them as the initialization of Ajustable Quantization in a fast train mode or you can generate them step by step.

##### Fast Train Mode
All the prerequired files are all ready, you can start training with Adjustable Quantization directly:
```
python train.py --weight_bits 8 \
                --act_bits 8 \
                --swish_bits 8 \
                --lr_fixed \
                --lr 3e-6 \
                --lr_bits 1e-3 \
                --alpha 7 \
                --beta 85 \
                --gamma 0.7 \
                --max_epoch 3 \
                --batch_size 25 \
                --bits_trainable \
                --iter_train \
                --dataset path_to_imagenet_tfrecord
```
***lr_fixed*** denotes using fixed learning rate and ***lr*** is learning rate for weights and quantization range, ***lr_bits*** is the learning rate for precision. ***alpha*** and ***beta*** are the relative weight of classification loss and metric loss. ***gamma*** is to control the gradient backward of Flops. ***max_epoch*** is the total epochs of training. ***iter_train*** enables our training strategy, i.e. enabling Adjustable Precision for 1 mini-epoch every 2 mini-epochs and fixing the precision in the other mini-epoch. The other parameters have the same usage as mentioned above.

#### 
